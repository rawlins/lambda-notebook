{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamblin semantics introductory fragment\n",
    "## Author: Kyle Rawlins\n",
    "### Updated: 3/10/24\n",
    "\n",
    "This is an implementation of compositional Hamblin semantics.  It is currently quite incomplete, but walks through the basic principles.  It also is useful for introducing some lexicon manipulation techniques in the lambda notebook (see also the appendix).\n",
    "\n",
    "Hamblin's influential idea is that, if the denotation of root declarative clause is a proposition, then the denotation of an interrogative can be thought of as a set of propositions, each corresponding to an answer to the question that interrogative would ask. For example, a polar interrogative \"Is it raining?\" would denote a set of two alternative propositions: the proposition that it is raining, and the proposition that it isn't. (See Hamblin 1973: [Questions in Montague English](https://www.jstor.org/stable/25000703), *Foundations of Language* 10.)\n",
    "\n",
    "A compositional Hamblin semantics is one that pushes this idea a bit further. Not only are clause denotations lifted to sets, any denotation during composition is lifted to a set. For ordinary cases, these sets may be singleton, but certain elements may then introduce or manipulate alternatives, leading to both question meanings, but also other compositional interactions of alternatives and operators. This idea was most famously developed in a 2002 paper by Kratzer and Shimoyama, [Indeterminate Pronouns: The View from Japanese](https://people.umass.edu/partee/RGGU_2004/Indeterminate%20Pronouns.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lambctl reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a type system with an intensional type\n",
    "type_e = types.type_e\n",
    "type_t = types.type_t\n",
    "type_n = types.type_n\n",
    "type_s = types.BasicType(\"s\")\n",
    "ts = meta.get_type_system()\n",
    "ts.add_atomic(type_s)\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: singleton set denotations\n",
    "\n",
    "As noted above, one core idea is that \"ordinary\" denotations are lifted to be singleton sets. When writing out a lexicon, therefore, where you previously would have written a regular element of some type $\\alpha$, you will now write a singleton set containing that element, which in the lambda notebook's type system is indicated by $\\{\\alpha\\}$. (In some approaches, this is alternatively modeled by using characteristic functions instead of sets, but here I take the direct implementation.)\n",
    "\n",
    "One could just do this manually, e.g. writing a denotation as a set: (n.b. the inner parentheses are required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lamb ||cat|| = {(L x_e : L w_s : Cat(w,x))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the present setting we can do this systematically. Essentially, to \"hamblinize\" the lexicon, we want to give a general procedure for converting ordinary meanings into singleton sets.\n",
    "\n",
    "The following cell defines a python function that will convert an arbitrary typed expression into a set. It then defines a transform that can be used when writing a lexical entry that applies the hamblinization operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamblinize_unit = %te L x_X : {x}\n",
    "hamblinize_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamblinize_unit(te(\"L x: Cat_<e,t>(x)\")).reduce_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply an operation like this when building a lexical item by setting an \"equality transform\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb.parsing.eq_transforms[\"hamblin\"] = hamblinize_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this in operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "||cat|| =<hamblin> L x_e : L w_s : Cat(w,x)\n",
    "||gray|| =<hamblin> L x_e : L w_s : Gray(w,x)\n",
    "||john|| =<hamblin> John_e\n",
    "x =<hamblin> L y_e : y\n",
    "||test|| = L x_e : Test(x) # don't hamblinize this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Pointwise Function Application\n",
    "\n",
    "Ordinary Function Application will not work on these sorts of lexical items, because they are not themselves functions. However, they contain functions. The standard approach to generalizing FA is to interpret it *pointwise* -- given a set of functions and arguments of the right types, apply each argument to each function and return the resulting set. I provide two versions of this operation below.\n",
    "\n",
    "First, we can look at the most general implementation of Pointwise Function Application (PFA), which is written using a combinator. The types `X` and `Y` are variables over arbitrary types.\n",
    "\n",
    "(This version of PFA characteristically results in complex but simplifiable expressions, that the lambda notebook doesn't itself simplify. In the long run, one would want to automatically simplify these expressions; but this would require a theorem prover that is currently beyond the scope of this project.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfa_combinator = %te L f_{<X,Y>} : L a_{X} : Set x_Y : Exists f1_<X,Y> : (Exists a1_X : (f1 << f & a1 << a) & x <=> f1(a1))\n",
    "pfa_combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = lang.td_system.copy()\n",
    "system.add_binary_rule(pfa_combinator, \"PFA\")\n",
    "lang.set_system(system)\n",
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john * cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulas of this style are pretty annoying to work with, and in practice most Hamblin Semantics examples work with listed sets, not sets determined by arbitrary predicates. The lambda notebook has a listed set implementation (the class `ListedSet`), and so for the simplified version of PFA I'll add support for this. The second version of PFA leaves any listed sets as listed sets, rather than lifting them to generalized sets. Essentially we special-case the PFA operation.\n",
    "\n",
    "Note that some limited reduction does occur with the $\\in$ operator; $x \\in \\{y \\:|\\: \\phi\\}$ gets converted to $(\\lambda y : \\phi)(x)$ and reduced, illustrated in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = te(\"x_e << (Set y_e : Test_<e,t>(y))\")\n",
    "r.simplify_all().derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfa_listed(fun, arg):\n",
    "    result = list()\n",
    "    # build every function-arg combination\n",
    "    for felem in fun.args:\n",
    "        for aelem in arg.args:\n",
    "            result.append(felem(aelem))\n",
    "    return meta.sets.sset(result)\n",
    "\n",
    "# generalized PFA: use the listed set code if it can work, otherwise use the more general combinator\n",
    "def pfa_general(fun, arg):\n",
    "    ts = meta.get_type_system()\n",
    "    general = pfa_combinator(fun)(arg) # do this for type checking\n",
    "    if isinstance(fun, meta.sets.ListedSet) and isinstance(arg, meta.sets.ListedSet):\n",
    "        return pfa_listed(fun, arg)\n",
    "    else:\n",
    "        return general.reduce_all()\n",
    "    \n",
    "system = lang.td_system.copy()\n",
    "system.add_binary_rule_uncurried(pfa_general, \"PFA\")\n",
    "lang.set_system(system)\n",
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john * cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denotations for interrogatives\n",
    "\n",
    "The above code has no alternative-sensitive denotations, so essentially reconstructs an ordinary compositional semantics with added set wrappers. This by itself is a bit pointless, but what we'll do next is add some alternative-sensitive items to the lexicon. In particular, the denotation of an interrogative pronoun like \"who\" in this system is treated as a set of individuals. The most general version of this would look something like ${x_e \\:|\\: Human(x)}$. However, for a more general introduction, we will use a listed set that fixes a specific domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "## To simplify, let's take there to only be three human-like entities in the universe.\n",
    "||who|| = {John_e, Mary_e, Sue_e}\n",
    "||left|| =<hamblin> L x_e : L w_s : Left(w,x)\n",
    "||saw|| =<hamblin> L x_e : L y_e : L w_s : Saw(w,y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the power of PFA is a bit more realized. The verb is still singleton, but it can combine with interrogative pronouns to produce non-singleton sets of alternatives. Intuitively, each of these alternative propositions is taken to correspond to a possible answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who * left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first-pass analysis for English, we interpret interrogative pronouns in-situ (their corresponding position in some other languages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(saw * who) # VP denotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with this can be described at an intuitive level as the alternatives \"percolating\" up the tree. Here's a fully composed sentence denotation with an object-position \"who\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john * (saw * who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(john * (saw * who)).tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple interrogatives are relative straighforward on this set of assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who * (saw * who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(who * (saw * who)).tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existentials and free choice\n",
    "\n",
    "So far, we have essentially reconstructed Hamblin's original semantics. Kratzer and Shimoyama propose that this system can also be used to account for existentials in Japanese, but treating them as consisting of an operator that takes scope, and an in-situ \"indeterminate\" pronoun that has the semantics illustrated above. In subsequent literature, this has been used to account for various \"free choice\" indefinites in many languages. Here I'll just present the core idea, using English lexical items.\n",
    "\n",
    "What we need to add is an alternative-aware operator that quantifies over set elements. Here are two possibilities from Kratzer and Shimoyama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "||HExists|| = L p_{<s,t>} : {(Lambda w_s  : Exists q_<s,t> : q(w) & (q << p))}\n",
    "||HForall|| = L p_{<s,t>} : {(Lambda w_s  : Forall q_<s,t> : (q << p) >> q(w))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operators compose via ordinary FA, and intuitively collapse alternative structure, resulting in a singleton denotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HExists * (who * (saw * john))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HExists * (john * (saw * who))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(HExists * (john * (saw * who))).tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various free choice items can also be analyzed using these techniques. I'll introduce the idea using English \"any\", though this is not probably the best candidate item for this kind of analysis.\n",
    "\n",
    "A puzzle about \"any\" is that it intuitively seems universal at some level, and in some contexts patterns as universal, but in others (e.g. in the scope of negation) it appears to be existential/indefinite. (There's a large literature on this that I'm not representing here.) A Hamblin account of such an item would say that it introduces alternatives like many other indefinites, but associates with a universal, not an existential operator. This doesn't by itself solve the licensing problem for \"any\" (it generally does require certain kinds of licensing contexts, e.g. negation, generics, etc), but does get the right interpretation across contexts. Here's a sample for a simple negative sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "||anyone|| = {John_e, Mary_e, Sue_e}\n",
    "||neg|| =<hamblin> L p_<s,t> : L w_s : ~p(w) # non-alternative-sensitive negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg * (john * (saw * anyone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HForall * (neg * (john * (saw * anyone)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general version\n",
    "\n",
    "Going back to the general version of the system, we don't want to actually define \"who\" extensionally in the general case. Here is how things would go with a more general implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "||who|| = Set x_e : Human(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, at the moment the lambda notebook doesn't do much simplification of these expressions, so they will be a bit unwieldy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cat * who).tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john * (saw * who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who * (saw * john)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HExists * (who * (saw * john))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook! There's lots more one can do with a compositional Hamblin semantics, and this idea has been the jumping off point for accounts of free choice, indefinites, and various complexities of interrogative semantics, as well as counter-proposals like inquisitive semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: another technique for Hamblinization\n",
    "\n",
    "Another way of Hamblinizing a lexicon would be to write extra magics for converting whole lexicons at once.  Here's a sketch of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamblinize_item(item):\n",
    "    \"\"\"Hamblinize a single lexical item.  Do so by building a singleton set out of it.\"\"\"\n",
    "    if meta.ts_compatible(item.type, meta.tp(\"{?}\")):\n",
    "        return item # assume that a set denotation is already Hamblinized\n",
    "    elif meta.ts_compatible(item.type, meta.tp(\"<{?},?>\")):\n",
    "        return item # Assume that a function whose domain is a set is an alternative-aware operator and shouldn't be modified\n",
    "\n",
    "    # build a new lang.Item object that wraps the content as a singleton set\n",
    "    new_i = item.copy()\n",
    "    new_i.content = meta.sets.sset([item.content])\n",
    "    return new_i\n",
    "\n",
    "# in the following two magics, only object language definitions are affected\n",
    "def h_magic(self, accum):\n",
    "    \"\"\"Hamblinize the accumulated definitions from a single cell, as a post-processing step\"\"\"\n",
    "\n",
    "    new_accum = lamb.magics.process_items(hamblinize_item, accum)[0]\n",
    "    self.env.update(new_accum)\n",
    "    return new_accum\n",
    "\n",
    "def h_magic_env(self):\n",
    "    \"\"\"Hamblinize an entire env\"\"\"\n",
    "\n",
    "    self.env.update(lamb.magics.process_items(hamblinize_item, self.env)[0]) # hamblinize every variable\n",
    "    self.shell.push(dict(self.env)) # export the new variables to the interactive shell\n",
    "    return lamb.parsing.html_output(self.env)\n",
    "\n",
    "lamb.magics.LambMagics.specials_post[\"hamblinize\"] = h_magic\n",
    "lamb.magics.LambMagics.specials[\"hamblinize_all\"] = h_magic_env\n",
    "\n",
    "type_s = types.BasicType(\"s\")\n",
    "ts = meta.get_type_system()\n",
    "ts.add_atomic(type_s)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb reset,hamblinize\n",
    "||cat|| = L x_e : L w_s : Cat(w,x)\n",
    "||gray|| = L x_e : L w_s : Gray(w,x)\n",
    "||john|| = J_e\n",
    "x = L y_e : y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lamb\n",
    "||test|| = L x_e : Test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifts any previous un-Hamblinized items to Hamblin sets\n",
    "%lambctl hamblinize_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
